# -*- coding: utf-8 -*-
"""Skin_Type_Predict_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XAlxpgWf_qcvEudOiR_BiPpLkl7CdD93
"""

# prompt: mount google drive

from google.colab import drive
drive.mount('/content/gdrive')

import keras, os
from keras.applications import VGG16
from tensorflow.keras.models import load_model
from keras.losses import categorical_crossentropy
from keras.layers import Dense, Flatten
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Model
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, EarlyStopping

import os
import numpy as np
import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator

# Define image paths
train_path = "/content/gdrive/MyDrive/Oil /classify all/train"
val_path = "/content/gdrive/MyDrive/Oil /classify all/valid"
test_path = "/content/gdrive/MyDrive/Oil /classify all/test"




# Define oiliness levels and descriptions
oiliness_levels = {
    "Dry": "Low Oiliness: Skin appears dry and matte.",
    "Normal": "Moderate Oiliness: Some shine or greasiness may be visible.",
    "Combination": "High Oiliness: Noticeable shine or greasiness on the skin.",
    "Oily": "Very High Oiliness: Skin appears excessively shiny and greasy."
}

# Define class labels (concise and unique)
class_labels = list(oiliness_levels.keys())  # Extract keys from oiliness_levels




# Create data generators with augmentation (optional)
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    rotation_range=180,  # Random rotation between -20 and 20 degrees
    brightness_range=[0.7, 1.3],  # Adjust brightness between 70% and 130%
    validation_split=0.1
)
val_datagen = ImageDataGenerator(rescale=1./255)

# Create data generators for training and validation sets
train_generator = train_datagen.flow_from_directory(
    train_path,
    target_size=(224, 224),
    batch_size=32,
    class_mode="categorical",
    subset="training"
)

val_generator = train_datagen.flow_from_directory(
    train_path,
    target_size=(224, 224),
    batch_size=64,
    class_mode="categorical",
    subset="validation"
)

# Initialize counters for each class
class_counters = {label: 0 for label in class_labels}
num_images_per_class = 2  # Number of images to display per class

# Show some images from each class
for i in range(len(train_generator)):
    images, labels = train_generator.next()
    for j in range(len(labels)):
        # Extract label and corresponding description
        label = train_generator.filenames[i*32 + j].split(os.path.sep)[-2]
        description = oiliness_levels[label]

        if class_counters[label] < num_images_per_class:
            plt.figure(figsize=(4, 4))
            plt.imshow(images[j])
            plt.title(f"Label: - {description}")  # Combine label and description
            plt.axis('off')
            plt.show()
            class_counters[label] += 1

    # Check if we have shown enough images from each class
    if all(count == num_images_per_class for count in class_counters.values()):
        break

# Load the pre-trained VGG16 model with weights from ImageNet
base_model = VGG16(weights="imagenet", include_top=False, input_shape=(224, 224, 3))

# Freeze the weights of all layers in the pre-trained model to prevent them from updating during training
for layer in base_model.layers:
    layer.trainable = False

# Define the input layer (same as the pre-trained model's input)
x = base_model.output

# Flatten the output of the pre-trained model
x = Flatten()(x)

from keras.regularizers import l2

# Add two dense layers with ReLU activation for feature extraction
x = Dense(units=1024, activation="relu", kernel_regularizer=l2(0.01))(x)  # Reduced number of units
x = Dense(units=512, activation="relu", kernel_regularizer=l2(0.01))(x)   # Reduced number of units

# Add a final dense layer with softmax activation for 4-class oiliness level prediction
predictions = Dense(units=4, activation="softmax")(x)

# Create the final model by connecting the base model's output to the custom layers
model = Model(inputs=base_model.input, outputs=predictions)

# Define the optimizer (Adam) and loss function (categorical crossentropy)
opt = Adam(learning_rate=1e-6)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

from keras.callbacks import ModelCheckpoint, EarlyStopping

checkpoint = ModelCheckpoint("vgg16_oilyness_detection.h5",
                             monitor='val_acc',
                             verbose=1,
                             save_best_only=True,
                             save_weights_only=False)  # Save the entire model

early_stopping = EarlyStopping(monitor='val_loss', patience=6) # Early stopping

hist = model.fit_generator(steps_per_epoch=len(train_generator),
                            generator=train_generator,
                            validation_data=val_generator,
                            validation_steps=len(val_generator),
                            epochs=100,
                           callbacks=[checkpoint, early_stopping])

import matplotlib.pyplot as plt

plt.plot(hist.history["accuracy"])
plt.plot(hist.history['val_accuracy'])
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title("Model Accuracy and Loss")
plt.ylabel("Value")
plt.xlabel("Epoch")
plt.legend(["Accuracy", "Validation Accuracy", "Loss", "Validation Loss"])
plt.show()

model.save("vgg16_oilyness_detection.h5")

# import dependencies
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import cv2
import numpy as np
import PIL
import io
import html
import time

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes

# initialize the Haar Cascade face detection model
face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)

  # get photo data
  data = eval_js('takePhoto({})'.format(quality))
  # get OpenCV format image
  img = js_to_image(data)
  # grayscale img
  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
  print(gray.shape)
  # get face bounding box coordinates using Haar Cascade
  faces = face_cascade.detectMultiScale(gray)
  # draw face bounding box on image
  for (x,y,w,h) in faces:
      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
  # save image
  cv2.imwrite(filename, img)

  return filename

try:
  filename = take_photo('photo.jpg')
  print('Saved to {}'.format(filename))

  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

from google.colab.patches import cv2_imshow

import cv2
import numpy as np

# Function to extract only the face region from the image
def extract_face(image, faces):
    if len(faces) == 0:
        return None

    # Get the bounding box coordinates of the first detected face
    x, y, w, h = faces[0]

    # Crop the image to extract only the face region
    face_image = image[y:y+h, x:x+w]

    return face_image

# Load the image
image = cv2.imread("/content/photo.jpg")

# Convert the image to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Load the pre-trained face cascade classifier
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

# Detect faces in the grayscale image
faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

# Extract only the face region from the image
face_image = extract_face(image, faces)

# Display the extracted face region
if face_image is not None:
    cv2_imshow(face_image)
else:
    print("No face detected in the image.")

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Assuming you have already extracted the face region and stored it in face_image

# Resizing the image for compatibility
image = cv2.resize(face_image, (500, 600))

# Convert the image to grayscale
image_bw = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Apply CLAHE
clahe = cv2.createCLAHE(clipLimit=5)
final_img = clahe.apply(image_bw) + 30

# Display the final image after CLAHE
cv2_imshow(final_img)

import cv2
import numpy as np
from google.colab.patches import cv2_imshow
from google.colab import drive

# Mount Google Drive
drive.mount('/content/gdrive')

# Load the pre-trained eye and mouth cascade classifiers
eye_cascade_path = "/content/gdrive/MyDrive/dat/haarcascade_eye.xml"
mouth_cascade_path = "/content/gdrive/MyDrive/dat/haarcascade_mcs_mouth.xml"

eye_cascade = cv2.CascadeClassifier(eye_cascade_path)
mouth_cascade = cv2.CascadeClassifier(mouth_cascade_path)

# Assuming you have already extracted the face region and stored it in final_img

# Detect eyes in the grayscale image
eyes = eye_cascade.detectMultiScale(final_img, 1.3, 5)

# Iterate through each eye and fill with black color
for (ex, ey, ew, eh) in eyes:
    cv2.rectangle(final_img, (ex, ey), (ex+ew, ey+eh), (0, 0, 0), -1)

# Detect mouth in the grayscale image
mouth = mouth_cascade.detectMultiScale(final_img, 1.5, 11)

# Iterate through each mouth and fill with black color
for (mx, my, mw, mh) in mouth:
    cv2.rectangle(final_img, (mx, my), (mx+mw, my+mh), (0, 0, 0), -1)

# Display the final image after detecting and filling eyes and mouth with black color
cv2_imshow(final_img)

# Convert final_img to integer array
final_img2 = final_img.astype(np.uint8)

# Thresholding to set pixel values less than 100 to 0 (black)
final_img2[final_img2 < 100] = 0

# Display the final image with pixel values less than 100 set to black
cv2_imshow(final_img2)

import numpy as np

# Assuming your image is stored in the variable final2_img
print("Shape of final_img2:", final_img2.shape)

import numpy as np

# Reshape the grayscale image to add a channel dimension
final_img2_reshaped = final_img2.reshape(final_img2.shape[0], final_img2.shape[1], 1)

# Repeat the single channel to create three channels (RGB)
final_img2_rgb = np.repeat(final_img2_reshaped, 3, axis=2)

# Check the shape of the resulting image
print("Shape of final_img2_rgb:", final_img2_rgb.shape)

from keras.models import load_model
from keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
import tempfile

# Load the trained model
trained_model = load_model('/content/vgg16_oilyness_detection.h5')

# Function to predict oiliness level for a given image
def predict_oiliness(image_array):
    # Extract non-zero pixels from the image array
    non_zero_pixels = image_array[image_array != 0]

    # Calculate the size of the resized array
    desired_size = 224 * 224 * 3

    # Resize the non-zero pixels to fit into the desired size
    resized_pixels = np.resize(non_zero_pixels, (desired_size,))

    # Reshape the resized pixels to match the input shape expected by the model
    reshaped_pixels = resized_pixels.reshape(224, 224, 3)

    # Make a prediction using the trained model
    prediction = trained_model.predict(np.expand_dims(reshaped_pixels, axis=0))

    # Get the predicted oiliness level
    predicted_oiliness_level = oiliness_levels[np.argmax(prediction)]

    return predicted_oiliness_level

# Example usage
predicted_oiliness = predict_oiliness(final_img2)

# Display the image
plt.imshow(final_img2)
plt.title(f"Predicted Oiliness Level: {predicted_oiliness}")
plt.axis('off')
plt.show()